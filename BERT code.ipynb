!pip install bert-score

import os
print(os.listdir(eng_annotation_path))
print(os.listdir(jap_annotation_path))
import os
from xml.etree import ElementTree

def extract_text_from_xml(xml_path, output_dir):
    tree = ElementTree.parse(xml_path)
    root = tree.getroot()
    
    # Extract all text from the XML
    text_content = []
    for text_element in root.findall(".//text"):
        
        text_content.append(text_element.text)

    # Join all the extracted text into a single string
    all_text = "\n".join(text_content)
    
    # Determine the output file path
    base_name = os.path.basename(xml_path)
    file_name = os.path.splitext(base_name)[0] + ".txt"
    output_path = os.path.join(output_dir, file_name)
    
    # Write the extracted text to a text file
    with open(output_path, "w", encoding="utf-8") as text_file:
        text_file.write(all_text)

def process_annotation_directory(annotation_dir, output_dir):
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Process each XML file in the directory
    for file_name in os.listdir(annotation_dir):
        if file_name.endswith(".xml"):
            xml_path = os.path.join(annotation_dir, file_name)
            extract_text_from_xml(xml_path, output_dir)
            break 

# Paths to the directories containing the XML files
eng_annotation_path = r"C:\Users\Lenovo\Downloads\annotations"
jap_annotation_path = r"C:\Users\Lenovo\Downloads\japanese annotations"

# Output directories for extracted text files
eng_output_dir = r"C:\Users\Lenovo\Downloads\extracted_annotations"
jap_output_dir = r"C:\Users\Lenovo\Downloads\extracted_japanese_annotations"

# Process the XML files in both directories
process_annotation_directory(eng_annotation_path, eng_output_dir)
process_annotation_directory(jap_annotation_path, jap_output_dir)

print("Text extraction complete.")

for file in os.listdir(eng_annotation_path):
    extract_text_from_xml(,file)
from bert_score import score
import os
import pandas as pd
from bert_score import score

def read_texts_from_directory(directory_path):
    texts = {}
    for file_name in os.listdir(directory_path):
        if file_name.endswith(".txt"):
            file_path = os.path.join(directory_path, file_name)
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read().strip()
                texts[file_name] = text
    return texts

# Paths to the directories containing the extracted texts
eng_extracted_dir = r"C:\Users\Lenovo\Downloads\extracted_annotations"
jap_extracted_dir = r"C:\Users\Lenovo\Downloads\extracted_japanese_annotations"

# Read English translations and Japanese references
english_translations = read_texts_from_directory(eng_extracted_dir)
japanese_references = read_texts_from_directory(jap_extracted_dir)

# Ensure both directories have the same number of files for comparison
if len(english_translations) != len(japanese_references):
    print("Warning: The number of English translations and Japanese references does not match.")
else:
    # Create an empty DataFrame to store the results
    results_df = pd.DataFrame(columns=['File Name', 'Precision', 'Recall', 'F1 Score'])
    
    # Calculate BERTScore for each file
    for key in sorted(english_translations.keys()):
        # Get translation and reference for the current file
        translation = [english_translations[key]]
        reference = [japanese_references[key]]
        
        # Calculate BERTScore using a multilingual model
        P, R, F1 = score(translation, reference, model_type='bert-base-multilingual-cased')

        # Create a DataFrame row for the current result
        row = pd.DataFrame({
            'File Name': [key],
            'Precision': [P[0].item()],
            'Recall': [R[0].item()],
            'F1 Score': [F1[0].item()]
        })

        # Append the row to the results DataFrame using pd.concat
        results_df = pd.concat([results_df, row], ignore_index=True)

    # Set the index of the DataFrame to 'File Name'
    results_df.set_index('File Name', inplace=True)
    
    # Output the DataFrame
    print(results_df)

    # Optionally, save the DataFrame to a CSV file
    results_df.to_csv(r"C:\Users\Lenovo\Downloads\bert_score_results.csv")

import os
import pandas as pd
from bert_score import score
import torch

def read_texts_from_directory(directory_path):
    texts = {}
    for file_name in os.listdir(directory_path):
        if file_name.endswith(".txt"):
            file_path = os.path.join(directory_path, file_name)
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read().strip()
                texts[file_name] = text
    return texts

# Paths to the directories containing the extracted texts
eng_extracted_dir = r"C:\Users\Lenovo\Downloads\extracted_annotations"
jap_extracted_dir = r"C:\Users\Lenovo\Downloads\extracted_japanese_annotations"

# Read English translations and Japanese references
english_translations = read_texts_from_directory(eng_extracted_dir)
japanese_references = read_texts_from_directory(jap_extracted_dir)

# Check if a GPU is available and use it if possible
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Ensure both directories have the same number of files for comparison
if len(english_translations) != len(japanese_references):
    print("Warning: The number of English translations and Japanese references does not match.")
else:
    # Create an empty DataFrame to store the results
    results_df = pd.DataFrame(columns=['File Name', 'Precision', 'Recall', 'F1 Score'])
    
    # Calculate BERTScore for each file
    for key in sorted(english_translations.keys()):
        # Get translation and reference for the current file
        translation = [english_translations[key]]
        reference = [japanese_references[key]]
        
        # Calculate BERTScore using a multilingual model, specifying device
        P, R, F1 = score(translation, reference, model_type='bert-base-multilingual-cased', device=device)

        # Create a DataFrame row for the current result
        row = pd.DataFrame({
            'File Name': [key],
            'Precision': [P[0].item()],
            'Recall': [R[0].item()],
            'F1 Score': [F1[0].item()]
        })

        # Append the row to the results DataFrame using pd.concat
        results_df = pd.concat([results_df, row], ignore_index=True)

    # Set the index of the DataFrame to 'File Name'
    results_df.set_index('File Name', inplace=True)
    
    # Output the DataFrame
    print(results_df)

    # Optionally, save the DataFrame to a CSV file
    results_df.to_csv(r"C:\Users\Lenovo\Downloads\bert_score_results.csv")

!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

